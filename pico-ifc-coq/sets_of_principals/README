This directory contains the Coq development corresponding to the set
of machines described in the paper "A Verified Information-Flow
Architecture" that use a sets-of-principals label model.

-- Contents

The biggest difference compared to the basic one is the verification
of the kernel code that manipulates sets (i.e. counted arrays). There
are also some differences in the semantics of the machines, which we
discuss now.

-- List of important files: 
   
   Instr.v 
           new instructions for system call, allocation, tag
           manipulating (privileged) instruction

   ConcreteMachine.v
   AbstractMachine.v
           to see what semantics we give to these instruction
           (some of them are privileged only)
           the system call is the most particular one.

   Lattices.v 
           as before, the interface of abstract lattices. 
	   now includes a definition of a lattice of sets of integers 
              (represented as Coq lists)
           
   Arrays.v 
           the code, specification, and proof of the implementation
           of counted array manipulation (subset, concatenation
           mainly)

   SOPCLattice.v
           what is a concrete lattice, 
           and a correct concrete lattice.
           In particular, the encoding and decoding of tags 
           are now relations, that involve the kernel memory. 
           Also show that the concrete implementation of sets of
	   integers (represented as arrays) is correct.


-- Compile
        make all
   This will compile the coq definitions and proofs.
   This code is known to work with Coq 8.4.

-- Glossary

   There are a few differences in terminology between this development and the paper:

   Development			Paper

   TMU ("Tag Management Unit")	Concrete machine hardware rule cache and its lookup mechanism
   Quasi-Abstract Machine	Symbolic rule machine
   tini_quasi_abstract_machine	Symbolic rule machine instantiated with rule table R^{abs}
   tini_concrete_machine	Concrete machine instantiated with fault handler generated from R^{abs}
   etc.

        
-- Summary:          

The machines defined here have a few more instructions. Most of those
are used to implement the kernel code for manipulating sets, and can't
be executed in user mode. For instance, there is an Alloc instruction,
used to allocate room for new sets. However, unlike the machines in
dyn_allocation, which use a sophisticated memory model, this allocator
simply extends the kernel memory at the end by some number of atoms.

The most significant addition to the semantics of the machines is a
system call mechanism (defined in Instr.v) for implementing
non-trivial operations in the concrete machine as a kernel services
(e.g. a joinP call, which we discuss below). We begin by adding a
SysCall instruction to the machines, which takes one argument (an
identifier). A system call is essentially made of (i) a relation
between stacks and (ii) a piece of code. The small-step relations for
the machines become parametric in a global table that maps identifiers
to system calls. In the abstract and symbolic rule machines, executing
the SysCall instruction looks for the corresponding identifier in the
global system call table, and uses the relation in the system call to
build the next machine state (c.f. step_syscall in
AbstractMachine.v). In the concrete machine, executing a system call
has the effect of pushing a return address on the stack and jumping to
some address in the kernel instruction memory (c.f. cstep_syscall in
ConcreteMachine.v). System calls are not subject to IFC rules like the
other instructions; instead, we assume that the relation that
implements a system call contains all the necessary checks for
ensuring proper IFC.

In our sets-of-principals label model, each label is a finite set of
integers (type Zset.t in Lattices.v); each integer represents a fixed,
preexisting principal. The label ordering is just set inclusion. We
provide a joinP system call that adds a principal to the label of an
atom, providing a form of classification.

In the concrete machine, sets of principals are represented by
integers that point to an array in kernel memory with the list of all
principals contained in that set. In order to program the fault
handler and the code of the joinP call, we implement basic array
operations such as subset testing and union. These are defined in file
Arrays.v, together with the proofs of their specifications. This
requires for instance the addition of a "repeat" construction for
generating loops (defined in CodeGen.v), as well as a Hoare logic
triple for it, proved in CodeSpecs.v. In addition to the fault handler
correctness proof, we also had to prove that the code that implements
the joinP call is correct with respect to the abstract semantics of
the system call. A system call table with joinP is defined in
SOPCLattice, together with the definition of the joinP code, as well 
as the proof of its specification.

The use of pointers as tags requires some interesting changes in the
formulation of noninterference. Indeed, since the interpretation of
tags is now relative to the kernel memory, an event of the concrete
machine needs to include the entire kernel memory in addition to the
tag. Once again, this memory is not visible to observers, but instead
define which events' payloads they are able to observe. To make the
interpretation of these tags in the statement of noninterference more
convenient, we change our notion of observation in TINI.v (the
Observation class) so that we convert raw events to a more abstract
type before matching them. This conversion corresponds to the
interpretation of tags in IFC terms.

The representation of labels in memory introduces another technical
difficulty: the relation between labels and tags now becomes partial,
whereas our basic label models used total functions (c.f. labToZ in
CLattices.v). This change makes some proofs more intricate.

